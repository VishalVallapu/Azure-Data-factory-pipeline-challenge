# Azure-Data-factory-pipeline-challenge
You recently started working as a data engineer at a company. Your first assignment is to use Azure Data Factory Pipelines to prepare log data for analysis in Azure Synapse Analytics. The log data must be available in a specific storage account, blob container, and folder for Azure Synapse Analytics.

Challenge includes:
1. Create the Necessary Linked Services
Create any linked services necessary to accomplish the objective of copying the log data to the output storage account.

2.Create the Necessary Datasets
Create any datasets necessary to accomplish the objective of copying the log data to the output storage account. The copied logs.txt data must end up being in a data folder in the output container of the outputcal storage account.

3.Create the Necessary Data FactoryÂ Pipeline

4. Manually Trigger the Pipeline
Manually trigger the pipeline to perform the copy operation. You must manually trigger the pipeline even if you previously performed a debug run of the pipeline.

5.Data Copied to Output Storage Container
Check if the data has been copied to the output storage container in the output storage account (storage account name beginning with outputcal).

